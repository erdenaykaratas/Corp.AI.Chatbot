#!/usr/bin/env python3
"""
Knowledge Processor - Metin ve tablo verilerini iÅŸler, anlamsal vektÃ¶rler oluÅŸturur
ve aranabilir bir FAISS indeksi (AI hafÄ±zasÄ±) inÅŸa eder.
"""
import os
import numpy as np
import pandas as pd
import faiss
from sentence_transformers import SentenceTransformer
import pickle
from fuzzywuzzy import fuzz

class KnowledgeProcessor:
    def __init__(self, model_name='paraphrase-multilingual-MiniLM-L12-v2'):
        """
        Bilgi iÅŸlemciyi baÅŸlatÄ±r ve embedding modelini yÃ¼kler.
        """
        print("ğŸ¤– Knowledge Processor baÅŸlatÄ±lÄ±yor...")
        self.model = SentenceTransformer(model_name)
        print(f"âœ… Embedding modeli '{model_name}' yÃ¼klendi.")
        self.index = None
        self.chunks = []
        self.store_variations = {}

    def _chunk_text(self, text: str, chunk_size=1000, overlap=150) -> list[str]:
        """
        Uzun bir metni, anlam bÃ¼tÃ¼nlÃ¼ÄŸÃ¼nÃ¼ koruyacak ÅŸekilde daha kÃ¼Ã§Ã¼k parÃ§alara ayÄ±rÄ±r.
        """
        if not isinstance(text, str):
            return []
            
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunks.append(text[start:end])
            start += chunk_size - overlap
        return chunks

    def process_knowledge_base(self, knowledge_base: dict, force_rebuild=False):
        """
        TÃ¼m bilgi tabanÄ±nÄ± (metinler, tablolar) iÅŸler, bir vektÃ¶r indeksi oluÅŸturur ve kaydeder.
        """
        index_path = 'faiss_index.bin'
        chunks_path = 'chunks.pkl'

        # Check if files exist and are not corrupted, unless forced to rebuild
        if not force_rebuild and os.path.exists(index_path) and os.path.exists(chunks_path):
            print("ğŸ’¾ KayÄ±tlÄ± AI hafÄ±zasÄ± (indeks ve metin parÃ§alarÄ±) bulunuyor, yÃ¼kleniyor...")
            try:
                self.index = faiss.read_index(index_path)
                with open(chunks_path, 'rb') as f:
                    self.chunks = pickle.load(f)
                print("âœ… AI hafÄ±zasÄ± baÅŸarÄ±yla yÃ¼klendi.")
                return
            except (EOFError, pickle.UnpicklingError) as e:
                print(f"âŒ KayÄ±tlÄ± dosya bozuk veya boÅŸ: {e}. Yeni hafÄ±za oluÅŸturulacak.")
                force_rebuild = True

        print("ğŸ§  Yeni bir AI hafÄ±zasÄ± oluÅŸturuluyor...")
        all_chunks = []
        
        for filename, content in knowledge_base.items():
            if isinstance(content, str):
                text_chunks = self._chunk_text(content)
                all_chunks.extend([f"Kaynak: {filename}\nÄ°Ã§erik: {chunk}" for chunk in text_chunks])

            elif isinstance(content, pd.DataFrame):
                for _, row in content.iterrows():
                    row_text = ", ".join([f"{col}: {val}" for col, val in row.items()])
                    all_chunks.append(f"Kaynak: {filename}\nÄ°Ã§erik: {row_text}")
                
                store_column = None
                for col in content.columns:
                    if 'maÄŸaza' in col.lower() or 'store' in col.lower():
                        store_column = col
                        break
                
                if store_column:
                    for index, row in content.iterrows():
                        store_name = str(row[store_column]).strip()
                        if store_name and store_name.lower() != 'nan':
                            row_data = {col: row[col] for col in content.columns if col != store_column}
                            row_text = f"MaÄŸaza: {store_name}, Veriler: {row_data}"
                            all_chunks.append(f"Kaynak: {filename}\nÄ°Ã§erik: {row_text}")
                            self.store_variations[store_name] = store_name
                            # Add simplified store name variations
                            base_name = store_name.split('-')[0].strip()
                            if base_name != store_name:
                                self.store_variations[base_name] = store_name
        
        if not all_chunks:
            print("âš ï¸ Ä°ÅŸlenecek veri bulunamadÄ±. HafÄ±za oluÅŸturulamadÄ±.")
            return

        print(f"ğŸ“„ Toplam {len(all_chunks)} adet metin parÃ§asÄ± (chunk) oluÅŸturuldu.")
        print("â³ VektÃ¶rler (embeddings) oluÅŸturuluyor...")
        
        embeddings = self.model.encode(all_chunks, show_progress_bar=True)
        
        print("âš¡ FAISS vektÃ¶r indeksi oluÅŸturuluyor...")
        d = embeddings.shape[1]
        self.index = faiss.IndexFlatL2(d)
        self.index.add(np.array(embeddings, dtype='float32'))
        self.chunks = all_chunks

        print(f"ğŸ’¾ AI hafÄ±zasÄ± '{index_path}' ve '{chunks_path}' dosyalarÄ±na kaydediliyor.")
        try:
            faiss.write_index(self.index, index_path)
            with open(chunks_path, 'wb') as f:
                pickle.dump(self.chunks, f)
            print("âœ… Yeni AI hafÄ±zasÄ± baÅŸarÄ±yla oluÅŸturuldu ve kaydedildi.")
        except Exception as e:
            print(f"âŒ HafÄ±za kaydedilirken hata: {e}")

    def search(self, query: str, k=5) -> list[str]:
        """
        Verilen bir sorgu iÃ§in anlamsal olarak en alakalÄ± metin parÃ§alarÄ±nÄ± bulur.
        """
        if self.index is None:
            return ["AI hafÄ±zasÄ± henÃ¼z oluÅŸturulmadÄ±."]
            
        query_words = query.lower().split()
        for store_name in self.store_variations.values():
            for word in query_words:
                if fuzz.partial_ratio(word, store_name.lower()) > 80:
                    query += f" {store_name}"
                    break
        
        query_embedding = self.model.encode([query])
        distances, indices = self.index.search(np.array(query_embedding, dtype='float32'), k)
        
        results = [self.chunks[i] for i in indices[0]]
        return results