#!/usr/bin/env python3
"""
Universal AI Assistant - TÃ¼m dosya tÃ¼rlerini (Excel, Word, PDF, CSV) okuyup anlayan geliÅŸmiÅŸ sistem
"""
import os
import json
import requests
import pandas as pd
import numpy as np
from flask import Flask, render_template, request, jsonify
from datetime import datetime
import re
from typing import Dict, List, Optional, Union
from thefuzz import fuzz

from data_loader import UniversalDataLoader
from knowledge_processor import KnowledgeProcessor
from nlp_processor import SmartNLPProcessor

print("ğŸ”§ DEBUG: main.py baÅŸlatÄ±lÄ±yor...")

try:
    from data_loader import UniversalDataLoader
    print("âœ… data_loader import edildi")
except Exception as e:
    print(f"âŒ data_loader import hatasÄ±: {e}")

try:
    from knowledge_processor import KnowledgeProcessor
    print("âœ… knowledge_processor import edildi")
except Exception as e:
    print(f"âŒ knowledge_processor import hatasÄ±: {e}")

try:
    from nlp_processor import SmartNLPProcessor
    print("âœ… nlp_processor import edildi")
except Exception as e:
    print(f"âŒ nlp_processor import hatasÄ±: {e}")

print(f"ğŸ”§ Ã‡alÄ±ÅŸma dizini: {os.getcwd()}")
try:
    print(f"ğŸ”§ Dizin iÃ§eriÄŸi: {os.listdir('.')}")
except Exception as e:
    print(f"âŒ Dizin listelenemedi: {e}")

if os.path.exists('company_data'):
    print(f"âœ… company_data klasÃ¶rÃ¼ mevcut")
    try:
        print(f"ğŸ“ Ä°Ã§eriÄŸi: {os.listdir('company_data')}")
    except Exception as e:
        print(f"âŒ company_data iÃ§eriÄŸi listelenemedi: {e}")
else:
    print("âŒ company_data klasÃ¶rÃ¼ yok!")

class UniversalAISystem:
    def __init__(self, data_directory: str):
        self.data_directory = data_directory
        self.data_loader = UniversalDataLoader()
        self.knowledge_proc = KnowledgeProcessor()
        self.nlp_proc = SmartNLPProcessor()
        self.knowledge_base = {}
        self.structured_data = {}
        self.text_documents = {}
        self.data_insights = {}
        self.initialize_system()
    
    def initialize_system(self):
        print("ğŸš€ Universal AI Assistant baÅŸlatÄ±lÄ±yor...")
        print("=" * 60)
        
        self.load_all_data()
        
        if self.knowledge_base:
            print(f"\nğŸ“š Toplam {len(self.knowledge_base)} dosya yÃ¼klendi:")
            print(f"   ğŸ“Š YapÄ±landÄ±rÄ±lmÄ±ÅŸ veri: {len(self.structured_data)} dosya")
            print(f"   ğŸ“„ Metin dokÃ¼manÄ±: {len(self.text_documents)} dosya")
            
            self.knowledge_proc.process_knowledge_base(self.knowledge_base)
            
            if self.knowledge_proc.index is not None:
                print("âœ… AI hafÄ±zasÄ± baÅŸarÄ±yla oluÅŸturuldu!")
                # Add store names to NLP processor
                for filename, insights in self.data_insights.items():
                    if 'store_rows' in insights:
                        self.nlp_proc.add_store_names(list(insights['store_rows'].keys()))
                        print(f"DEBUG: Indexed stores in {filename}: {list(insights['store_rows'].keys())}")
                self.analyze_data_insights()
                self.test_system_capabilities()
            else:
                print("âŒ AI hafÄ±zasÄ± oluÅŸturulamadÄ±!")
        else:
            print("âŒ HiÃ§bir dosya yÃ¼klenemedi!")
    
    def load_all_data(self):
        print(f"ğŸ“‚ '{self.data_directory}' klasÃ¶rÃ¼nden dosyalar yÃ¼kleniyor...")
        
        if not os.path.exists(self.data_directory):
            print(f"âŒ Veri klasÃ¶rÃ¼ bulunamadÄ±: '{self.data_directory}'")
            return
        
        file_stats = {'excel': 0, 'csv': 0, 'pdf': 0, 'word': 0, 'other': 0}
        
        for filename in os.listdir(self.data_directory):
            file_path = os.path.join(self.data_directory, filename)
            
            if not os.path.isfile(file_path):
                continue
            
            print(f"\nğŸ“ Ä°ÅŸleniyor: {filename}")
            file_ext = os.path.splitext(filename)[1].lower()
            
            try:
                data = self.data_loader.load_data(file_path)
                
                if data is not None:
                    self.knowledge_base[filename] = data
                    
                    if file_ext in ['.xlsx', '.xls']:
                        self._process_excel_data(filename, data)
                        file_stats['excel'] += 1
                    elif file_ext == '.csv':
                        self._process_csv_data(filename, data)
                        file_stats['csv'] += 1
                    elif file_ext == '.pdf':
                        self._process_pdf_data(filename, data)
                        file_stats['pdf'] += 1
                    elif file_ext == '.docx':
                        self._process_word_data(filename, data)
                        file_stats['word'] += 1
                    else:
                        file_stats['other'] += 1
                    
                    print(f"   âœ… BaÅŸarÄ±yla yÃ¼klendi!")
                else:
                    print(f"   âŒ YÃ¼klenemedi!")
                    
            except Exception as e:
                print(f"   âŒ Hata: {str(e)}")
        
        print(f"\nğŸ“ˆ YÃœKLEME Ä°STATÄ°STÄ°KLERÄ°:")
        print(f"   ğŸ“Š Excel dosyasÄ±: {file_stats['excel']}")
        print(f"   ğŸ“‹ CSV dosyasÄ±: {file_stats['csv']}")
        print(f"   ğŸ“„ PDF dosyasÄ±: {file_stats['pdf']}")
        print(f"   ğŸ“ Word dosyasÄ±: {file_stats['word']}")
        print(f"   ğŸ“¦ DiÄŸer: {file_stats['other']}")
    
    def _process_excel_data(self, filename: str, data: pd.DataFrame):
        try:
            insights = {
                'type': 'excel',
                'shape': data.shape,
                'columns': list(data.columns),
                'numeric_columns': list(data.select_dtypes(include=[np.number]).columns),
                'date_columns': [],
                'text_columns': list(data.select_dtypes(include=['object']).columns),
                'sample_data': data.head(3).to_dict('records') if len(data) > 0 else [],
                'summary': {},
                'store_rows': {},
                'department_counts': {}
            }
            
            for col in data.columns:
                if any(keyword in col.lower() for keyword in ['tarih', 'date', 'gun', 'day', 'time']):
                    insights['date_columns'].append(col)
            
            for col in insights['numeric_columns']:
                insights['summary'][col] = {
                    'min': float(data[col].min()) if not data[col].empty else 0,
                    'max': float(data[col].max()) if not data[col].empty else 0,
                    'mean': float(data[col].mean()) if not data[col].empty else 0,
                    'sum': float(data[col].sum()) if not data[col].empty else 0
                }
            
            store_column = None
            for col in data.columns:
                if 'maÄŸaza' in col.lower() or 'store' in col.lower():
                    store_column = col
                    break
            
            if store_column:
                data[store_column] = data[store_column].astype(str).str.strip()  # Ensure store names are strings and clean
                for index, row in data.iterrows():
                    store_name = row[store_column]
                    if store_name and store_name.lower() != 'nan':
                        insights['store_rows'][store_name] = {
                            col: row[col] for col in data.columns if col != store_column
                        }
                print(f"DEBUG: Loaded {len(insights['store_rows'])} stores from {filename}: {list(insights['store_rows'].keys())}")
            
            dept_column = None
            for col in data.columns:
                if 'departman' in col.lower():
                    dept_column = col
                    break
            
            if dept_column:
                insights['department_counts'] = data[dept_column].value_counts().to_dict()
            
            self.structured_data[filename] = data
            self.data_insights[filename] = insights
            
            print(f"   ğŸ“Š {data.shape[0]} satÄ±r, {data.shape[1]} sÃ¼tun")
            print(f"   ğŸ”¢ SayÄ±sal sÃ¼tun: {len(insights['numeric_columns'])}")
            print(f"   ğŸ“… Tarih sÃ¼tunu: {len(insights['date_columns'])}")
            if store_column:
                print(f"   ğŸ¬ MaÄŸaza sÃ¼tunu: {store_column}, {len(insights['store_rows'])} maÄŸaza indekslendi")
            if dept_column:
                print(f"   ğŸ‘¥ Departman sÃ¼tunu: {dept_column}, {len(insights['department_counts'])} departman bulundu")
            
        except Exception as e:
            print(f"   âš ï¸ Excel analiz hatasÄ±: {e}")
    
    def _process_csv_data(self, filename: str, data: pd.DataFrame):
        self._process_excel_data(filename, data)
        self.data_insights[filename]['type'] = 'csv'
    
    def _process_pdf_data(self, filename: str, data: str):
        insights = {
            'type': 'pdf',
            'char_count': len(data),
            'word_count': len(data.split()),
            'line_count': len(data.split('\n')),
            'has_numbers': bool(re.search(r'\d+', data)),
            'has_dates': bool(re.search(r'\d{1,2}[/-]\d{1,2}[/-]\d{2,4}', data)),
            'preview': data[:200] + "..." if len(data) > 200 else data
        }
        self.text_documents[filename] = data
        self.data_insights[filename] = insights
        print(f"   ğŸ“„ {insights['word_count']} kelime, {insights['line_count']} satÄ±r")
    
    def _process_word_data(self, filename: str, data: str):
        self._process_pdf_data(filename, data)
        self.data_insights[filename]['type'] = 'word'
    
    def analyze_data_insights(self):
        print(f"\nğŸ§  VERÄ° ANALÄ°ZÄ°:")
        if self.structured_data:
            total_rows = sum(df.shape[0] for df in self.structured_data.values())
            print(f"   ğŸ“Š Toplam veri satÄ±rÄ±: {total_rows:,}")
            all_columns = []
            for insights in self.data_insights.values():
                if insights['type'] in ['excel', 'csv']:
                    all_columns.extend(insights['columns'])
            common_patterns = {}
            for col in all_columns:
                col_lower = col.lower()
                for pattern in ['tarih', 'date', 'satis', 'sales', 'tutar', 'amount', 'magaza', 'store', 'departman']:
                    if pattern in col_lower:
                        common_patterns[pattern] = common_patterns.get(pattern, 0) + 1
            if common_patterns:
                print(f"   ğŸ” Ortak veri tÃ¼rleri: {dict(list(common_patterns.items())[:5])}")
        if self.text_documents:
            total_words = sum(self.data_insights[f]['word_count'] 
                             for f in self.text_documents.keys() 
                             if f in self.data_insights)
            print(f"   ğŸ“ Toplam metin kelimesi: {total_words:,}")
    
    def test_system_capabilities(self):
        print(f"\nğŸ”§ SÄ°STEM YETENEKLERÄ° TESTÄ°:")
        test_queries = [
            "kaÃ§ adet excel dosyasÄ± var",
            "hangi dosyalarda satÄ±ÅŸ verisi bulunuyor",
            "toplam kaÃ§ satÄ±r veri var",
            "en bÃ¼yÃ¼k sayÄ±sal deÄŸer nedir",
            "tarih sÃ¼tunu olan dosyalar hangileri",
            "Ä°STÄ°NYEPARK - NSP maÄŸazasÄ±nÄ±n 2025 satÄ±ÅŸ verisi nedir",
            "sales.xlsx dosyasÄ±ndaki tÃ¼m maÄŸazalarÄ±n bÃ¼yÃ¼me oranlarÄ±nÄ± karÅŸÄ±laÅŸtÄ±r",
            "Departman grafiÄŸi gÃ¶ster"
        ]
        for query in test_queries:
            print(f"ğŸ” Test: '{query}'")
            results = self.knowledge_proc.search(query, k=2)
            if results and "AI hafÄ±zasÄ± henÃ¼z oluÅŸturulmadÄ±" not in results[0]:
                print("  âœ… Ä°lgili bilgi bulundu!")
            else:
                print("  âŒ SonuÃ§ yok")
    
    def _generate_smart_answer(self, query: str, context_chunks: List[str]):
        print(f"ğŸ¤– AkÄ±llÄ± cevap Ã¼retiliyor: '{query[:50]}...'")
        query_lower = query.lower()
        is_data_query = any(keyword in query_lower for keyword in [
            'kaÃ§', 'toplam', 'en Ã§ok', 'en bÃ¼yÃ¼k', 'en kÃ¼Ã§Ã¼k', 'ortalama', 'maksimum', 'minimum',
            'satÄ±ÅŸ', 'sales', 'tutar', 'amount', 'maÄŸaza', 'store', 'tarih', 'date', 'departman', 'bÃ¼yÃ¼me', 'growth'
        ])
        is_chart_query = 'grafik' in query_lower or 'gÃ¶ster' in query_lower
        is_growth_query = 'bÃ¼yÃ¼me' in query_lower or 'growth' in query_lower

        # Use NLP processor to extract entities
        entities = self.nlp_proc.predict_intent(query, self.data_insights)
        store_name = entities['entities']['stores'][0] if entities['entities']['stores'] else None

        enriched_context = context_chunks[:5]
        response_data = {'response': '', 'chart': None}

        # Handle growth rate comparison for all stores
        if is_growth_query and 'sales.xlsx' in self.data_insights:
            sales_insights = self.data_insights['sales.xlsx']
            if 'store_rows' in sales_insights:
                growth_rates = {}
                for store, data in sales_insights['store_rows'].items():
                    try:
                        sales_2024 = float(data.get('Ciro 2024 (TRY-KDV siz)', 0))
                        sales_2025 = float(data.get('Ciro 2025 (TRY-KDV siz)', 0))
                        growth_rate = float(data.get('Ciro % BÃ¼yÃ¼me(24den25e)', 0))
                        if sales_2024 > 0:  # Verify growth rate
                            calculated_growth = ((sales_2025 - sales_2024) / sales_2024) * 100
                            growth_rates[store] = calculated_growth
                        else:
                            growth_rates[store] = growth_rate  # Use provided growth rate if 2024 sales are 0
                    except (ValueError, TypeError) as e:
                        print(f"DEBUG: Error processing store {store}: {e}")
                        growth_rates[store] = 0  # Default to 0 if data is invalid
                
                print(f"DEBUG: Growth rates calculated for {len(growth_rates)} stores: {growth_rates}")
                
                if growth_rates:
                    # Sort stores by growth rate
                    sorted_growth = sorted(growth_rates.items(), key=lambda x: x[1], reverse=True)
                    top_5_stores = sorted_growth[:5]  # Top 5 stores
                    
                    response_text = (
                        f"En Ã§ok bÃ¼yÃ¼me gÃ¶steren 5 maÄŸaza:\n" +
                        "\n".join([f"{i+1}. **{store}**: %{rate:.2f}" for i, (store, rate) in enumerate(top_5_stores)])
                    )
                    if len(sorted_growth) > 5:
                        response_text += "\n... (DiÄŸer maÄŸazalar iÃ§in tam liste talep edebilirsiniz.)"

                    # Generate chart for top 5 stores
                    if is_chart_query:
                        response_data['chart'] = {
                            'type': 'bar',
                            'title': '2024-2025 En Ã‡ok BÃ¼yÃ¼yen 5 MaÄŸaza',
                            'data': {
                                'labels': [store for store, _ in top_5_stores],
                                'data': [rate for _, rate in top_5_stores],
                                'backgroundColor': ['#4CAF50', '#2196F3', '#FFC107', '#F44336', '#9C27B0'],
                                'borderColor': ['#388E3C', '#1976D2', '#FFB300', '#D32F2F', '#7B1FA2'],
                                'borderWidth': 1
                            },
                            'options': {
                                'scales': {
                                    'y': {
                                        'title': {'display': True, 'text': 'BÃ¼yÃ¼me OranÄ± (%)'},
                                        'beginAtZero': True
                                    },
                                    'x': {
                                        'title': {'display': True, 'text': 'MaÄŸaza AdÄ±'}
                                    }
                                }
                            }
                        }

                    response_data['response'] = response_text
                    return response_data
                else:
                    response_data['response'] = "BÃ¼yÃ¼me oranÄ± hesaplanacak yeterli veri bulunamadÄ±."
                    return response_data

        # Handle department chart query
        if is_chart_query and 'departman' in query_lower:
            department_data = {}
            for filename, insights in self.data_insights.items():
                if insights['type'] in ['excel', 'csv'] and 'department_counts' in insights:
                    department_data.update(insights['department_counts'])
            if department_data:
                response_data['chart'] = {
                    'type': 'pie',
                    'title': 'Departmanlara GÃ¶re Ã‡alÄ±ÅŸan SayÄ±sÄ±',
                    'data': {
                        'labels': list(department_data.keys()),
                        'data': list(department_data.values()),
                        'backgroundColor': ['#4CAF50', '#2196F3', '#FFC107', '#F44336'],
                        'borderColor': ['#388E3C', '#1976D2', '#FFB300', '#D32F2F'],
                        'borderWidth': 1
                    }
                }
                response_data['response'] = (
                    "Departmanlara gÃ¶re Ã§alÄ±ÅŸan sayÄ±larÄ±:\n" +
                    "\n".join([f"{dept}: {count} Ã§alÄ±ÅŸan" for dept, count in department_data.items()])
                )
                return response_data
            else:
                response_data['response'] = "Departman verisi bulunamadÄ±."
                return response_data

        # Handle specific store queries
        if is_data_query and store_name:
            for filename, insights in self.data_insights.items():
                if insights['type'] in ['excel', 'csv'] and 'store_rows' in insights:
                    if store_name in insights['store_rows']:
                        store_data = insights['store_rows'][store_name]
                        enriched_context.append(
                            f"MAÄAZA: {store_name}\n" +
                            "\n".join([f"{key}: {value}" for key, value in store_data.items()])
                        )

        # General query processing with API
        context_string = "\n\n".join(enriched_context)
        data_summary = self._create_data_summary()
        prompt = f"""Sen evrensel bir veri analisti ve dokÃ¼man uzmanÄ±sÄ±n. FarklÄ± tÃ¼rdeki dosyalarÄ± (Excel, Word, PDF, CSV) analiz edip kullanÄ±cÄ±nÄ±n sorularÄ±na cevap veriyorsun.

KULLANICININ SORUSU: "{query}"

MEVCUT BÄ°LGÄ°LER VE VERÄ°LER:
{context_string}

VERÄ° Ã–ZETÄ°:
{data_summary}

GÃ–REVLER:
1. EÄŸer sorgu sayÄ±sal/istatistiksel ise, eldeki verilerden hesaplamalar yap
2. EÄŸer sorguda bir maÄŸaza adÄ± varsa, o maÄŸazanÄ±n satÄ±rÄ±ndaki verileri (Ã¶r. satÄ±ÅŸ, bÃ¼yÃ¼me) dÃ¶ndÃ¼r
3. EÄŸer sorgu bÃ¼yÃ¼me oranÄ± iÃ§eriyorsa, tÃ¼m maÄŸazalarÄ±n 2024-2025 bÃ¼yÃ¼me oranlarÄ±nÄ± hesapla ve en yÃ¼kseÄŸini belirt
4. EÄŸer sorgu grafik iÃ§eriyorsa, uygun verileri Ã¶zetle
5. MÃ¼mkÃ¼nse spesifik sayÄ±lar, dosya isimleri ve detaylar sun
6. TÃ¼rkÃ§e, anlaÅŸÄ±lÄ±r ve doÄŸrudan cevap ver
7. EÄŸer tam bilgi yoksa, eldeki bilgilere dayanarak en iyi tahmini yap

CEVAP:"""
        api_key = "AIzaSyD-nx8kwlZz2nmVLgcrN2rgcwQKId0Duh8"
        api_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}"
        
        payload = {"contents": [{"role": "user", "parts": [{"text": prompt}]}]}
        headers = {'Content-Type': 'application/json'}
        try:
            response = requests.post(api_url, headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            result = response.json()
            if 'error' in result:
                response_data['response'] = f"Cevap Ã¼retilirken hata: {result['error'].get('message', 'Bilinmeyen hata')}"
                return response_data
            if ('candidates' in result and result['candidates'] and
                'content' in result['candidates'][0] and
                'parts' in result['candidates'][0]['content'] and
                result['candidates'][0]['content']['parts']):
                generated_text = result['candidates'][0]['content']['parts'][0]['text']
                print("âœ… AkÄ±llÄ± cevap Ã¼retildi!")
                response_data['response'] = generated_text.strip()
                return response_data
            else:
                response_data['response'] = "Bu soru iÃ§in uygun cevap Ã¼retemiyorum."
                return response_data
        except Exception as e:
            print(f"âŒ LLM hatasÄ±: {e}")
            response_data['response'] = f"Cevap Ã¼retilirken teknik bir sorun oluÅŸtu: {str(e)}"
            return response_data
    
    def _create_data_summary(self):
        summary_parts = []
        file_types = {}
        for filename, insights in self.data_insights.items():
            file_type = insights['type']
            file_types[file_type] = file_types.get(file_type, 0) + 1
        summary_parts.append(f"Dosya tÃ¼rleri: {dict(file_types)}")
        if self.structured_data:
            total_rows = sum(df.shape[0] for df in self.structured_data.values())
            total_cols = sum(df.shape[1] for df in self.structured_data.values())
            summary_parts.append(f"Toplam veri: {total_rows} satÄ±r, {total_cols} sÃ¼tun")
            numeric_summaries = []
            for filename, insights in self.data_insights.items():
                if insights['type'] in ['excel', 'csv'] and 'summary' in insights:
                    for col, stats in insights['summary'].items():
                        numeric_summaries.append(f"{filename}-{col}: {stats}")
            if numeric_summaries:
                summary_parts.append(f"SayÄ±sal veriler: {numeric_summaries[:3]}")
        if self.text_documents:
            total_words = sum(self.data_insights[f]['word_count'] 
                             for f in self.text_documents.keys() 
                             if f in self.data_insights)
            summary_parts.append(f"Toplam metin: {total_words} kelime")
        return " | ".join(summary_parts)
    
    def process_universal_query(self, user_query: str):
        print(f"\nğŸ” Evrensel sorgu iÅŸleniyor: '{user_query}'")
        relevant_chunks = self.knowledge_proc.search(user_query, k=32)  # Ensure all stores are covered
        if not relevant_chunks or "AI hafÄ±zasÄ± henÃ¼z oluÅŸturulmadÄ±" in relevant_chunks[0]:
            return {'response': "Sistemde henÃ¼z veri yÃ¼klenmemiÅŸ veya arama yapÄ±lamÄ±yor.", 'chart': None}
        print(f"ğŸ“‹ {len(relevant_chunks)} ilgili bilgi parÃ§asÄ± bulundu")
        final_response = self._generate_smart_answer(user_query, relevant_chunks)
        return final_response
    
    def get_system_status(self):
        return {
            'total_files': len(self.knowledge_base),
            'structured_data_files': len(self.structured_data),
            'text_document_files': len(self.text_documents),
            'ai_memory_ready': self.knowledge_proc.index is not None,
            'total_chunks': len(self.knowledge_proc.chunks) if self.knowledge_proc.chunks else 0,
            'file_types': {filename: insights['type'] for filename, insights in self.data_insights.items()},
            'data_summary': self._create_data_summary() if self.data_insights else "Veri yok"
        }

app = Flask(__name__)

print("ğŸŒŸ UNIVERSAL AI ASSISTANT BAÅLATILIYOR...")
DATA_FOLDER = "company_data"
universal_system = UniversalAISystem(DATA_FOLDER)

if universal_system.knowledge_proc.index is not None:
    print("âœ… UNIVERSAL AI ASSISTANT HAZIR!")
else:
    # Bu mesajÄ± gÃ¶rÃ¼yorsanÄ±z, Render'daki dosya yollarÄ±nda veya dosya iÃ§eriÄŸinde bir sorun olabilir.
    print("âŒ SISTEM BAÅLATILAMADI! LÃ¼tfen 'company_data' klasÃ¶rÃ¼nÃ¼ ve dosyalarÄ±nÄ± kontrol edin.")


@app.route('/')
def index():
    return render_template('index.html')

@app.route('/api/status')
def status():
    # 'universal_system' artÄ±k global alanda tanÄ±mlandÄ±ÄŸÄ± iÃ§in 'global' anahtar kelimesine gerek yok.
    if universal_system is None:
        return jsonify({'error': 'Sistem henÃ¼z baÅŸlatÄ±lmadÄ±'}), 503
    return jsonify(universal_system.get_system_status())

@app.route('/api/query', methods=['POST'])
def query():
    # 'universal_system' artÄ±k global alanda tanÄ±mlandÄ±ÄŸÄ± iÃ§in 'global' anahtar kelimesine gerek yok.
    if universal_system is None or universal_system.knowledge_proc.index is None:
        return jsonify({'error': 'Sistem henÃ¼z hazÄ±r deÄŸil.'}), 503
    
    data = request.get_json()
    user_query = data.get('query', '').strip()
    if not user_query:
        return jsonify({'error': 'BoÅŸ sorgu'}), 400
    
    try:
        result = universal_system.process_universal_query(user_query)
        return jsonify(result)
    except Exception as e:
        print(f"âŒ Sorgu hatasÄ±: {e}")
        return jsonify({'error': f'Hata: {str(e)}'}), 500

# Bu blok, dosyayÄ± doÄŸrudan `python main.py` ile Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zda (lokal geliÅŸtirme iÃ§in) devreye girer.
# Gunicorn gibi production sunucularÄ± bu bloÄŸu Ã§alÄ±ÅŸtÄ±rmaz, bunun yerine yukarÄ±daki 'app' nesnesini kullanÄ±r.
if __name__ == '__main__':
    print("\n" + "=" * 60)
    print("ğŸŒ Lokal geliÅŸtirme sunucusu baÅŸlatÄ±lÄ±yor...")
    print("   TarayÄ±cÄ±: http://localhost:5000")
    print("   Durum: http://localhost:5000/api/status")
    print("=" * 60)
    # Production'da Gunicorn zaten kendi ayarlarÄ±nÄ± kullanacaÄŸÄ± iÃ§in buradaki host/port/debug
    # sadece lokal geliÅŸtirme iÃ§indir. debug=False kullanmak daha gÃ¼venlidir.
    app.run(host='0.0.0.0', port=5000, debug=False)