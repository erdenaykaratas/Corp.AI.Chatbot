#!/usr/bin/env python3
"""
AI Assistant Server - RAG mimarisi ve LLM ile ÅŸirket verilerini anlar ve yanÄ±tlar.
"""
import os
import json
import requests
import pandas as pd
from flask import Flask, render_template, request, jsonify

# Gerekli sÄ±nÄ±flarÄ± ayrÄ± dosyalardan import ediyoruz
from data_loader import UniversalDataLoader
from knowledge_processor import KnowledgeProcessor

class AIAssistantServer:
    """
    TÃ¼m veri kaynaklarÄ±nÄ± yÃ¶neten, RAG ve LLM kullanarak sorgularÄ± iÅŸleyen ana sunucu sÄ±nÄ±fÄ±.
    """
    def __init__(self, data_directory: str):
        self.data_directory = data_directory
        self.data_loader = UniversalDataLoader()
        self.knowledge_proc = KnowledgeProcessor()
        self.knowledge_base = {}
        self.load_knowledge_base()

        if self.knowledge_base:
            self.knowledge_proc.process_knowledge_base(self.knowledge_base)
        else:
            print("âŒ Bilgi tabanÄ± boÅŸ olduÄŸu iÃ§in AI hafÄ±zasÄ± oluÅŸturulamadÄ±.")

    def load_knowledge_base(self):
        print(f"ğŸ“š Bilgi tabanÄ± '{self.data_directory}' klasÃ¶rÃ¼nden yÃ¼kleniyor...")
        if not os.path.exists(self.data_directory):
            print(f"âŒ Veri klasÃ¶rÃ¼ bulunamadÄ±: '{self.data_directory}'. LÃ¼tfen oluÅŸturun.")
            return
        for filename in os.listdir(self.data_directory):
            file_path = os.path.join(self.data_directory, filename)
            if os.path.isfile(file_path):
                data = self.data_loader.load_data(file_path)
                if data is not None:
                    self.knowledge_base[filename] = data
        if self.knowledge_base:
            print(f"âœ… Bilgi tabanÄ± baÅŸarÄ±yla yÃ¼klendi. {len(self.knowledge_base)} dosya iÅŸlendi.")

    def _generate_answer_with_llm(self, query, context_chunks):
        """
        BÃ¼yÃ¼k Dil Modeli (LLM) kullanarak bulunan bilgilere gÃ¶re bir cevap Ã¼retir.
        (Hata yÃ¶netimi ile saÄŸlamlaÅŸtÄ±rÄ±lmÄ±ÅŸ versiyon)
        """
        print("ğŸ¤– LLM ile cevap Ã¼retiliyor...")
        
        context_string = "\n---\n".join(context_chunks)
        prompt = f"""
        Sen bir ÅŸirket iÃ§i yardÄ±m asistanÄ±sÄ±n. GÃ¶revin, sana verilen bilgileri kullanarak kullanÄ±cÄ±nÄ±n sorusuna kÄ±sa, net ve doÄŸrudan bir cevap vermektir.
        
        KullanÄ±cÄ±nÄ±n Sorusu: "{query}"
        
        Bu soruya cevap vermek iÃ§in kullanabileceÄŸin bilgiler:
        ---
        {context_string}
        ---
        
        YukarÄ±daki bilgileri dikkatlice analiz et ve sadece bu bilgilere dayanarak soruyu cevapla. EÄŸer verilen bilgiler soruyu cevaplamak iÃ§in yetersizse, "Bu soruya cevap verecek yeterli bilgiye sahip deÄŸilim." de. Kendi bilgini veya varsayÄ±mlarÄ±nÄ± katma.
        """

        api_key = "AIzaSyD-nx8kwlZz2nmVLgcrN2rgcwQKId0Duh8" # Canvas ortamÄ± tarafÄ±ndan otomatik saÄŸlanacaktÄ±r.
        api_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}"
        
        payload = {"contents": [{"role": "user", "parts": [{"text": prompt}]}]}
        headers = {'Content-Type': 'application/json'}

        try:
            response = requests.post(api_url, headers=headers, json=payload, timeout=60)
            response.raise_for_status() # HTTP 4xx/5xx hatalarÄ±nÄ± kontrol et
            
            result = response.json()
            
            if 'error' in result:
                error_details = result['error']
                print(f"âŒ LLM API HatasÄ±: {error_details.get('message', 'Bilinmeyen hata')}")
                return "Cevap Ã¼retilirken modelden bir hata alÄ±ndÄ±. Detaylar iÃ§in sunucu loglarÄ±nÄ± kontrol edin."

            if ('candidates' in result and result['candidates'] and
                'content' in result['candidates'][0] and
                'parts' in result['candidates'][0]['content'] and
                result['candidates'][0]['content']['parts']):
                
                generated_text = result['candidates'][0]['content']['parts'][0]['text']
                return generated_text.strip()
            else:
                print(f"âš ï¸ LLM'den geÃ§erli bir aday cevap alÄ±namadÄ±. YanÄ±t: {result}")
                return "Model bu soru iÃ§in bir cevap Ã¼retemedi. LÃ¼tfen sorunuzu farklÄ± bir ÅŸekilde ifade etmeyi deneyin."

        except requests.exceptions.HTTPError as e:
            # HTTP hatalarÄ±nÄ± daha spesifik olarak yakala
            print(f"âŒ LLM API HatasÄ±: Status {e.response.status_code}, YanÄ±t: {e.response.text}")
            if e.response.status_code == 403:
                return "Cevap Ã¼retilemedi. API anahtarÄ± geÃ§ersiz veya gerekli izinlere sahip deÄŸil gibi gÃ¶rÃ¼nÃ¼yor. (Hata Kodu: 403)"
            elif e.response.status_code == 429:
                return "API kullanÄ±m limiti aÅŸÄ±ldÄ±. LÃ¼tfen bir sÃ¼re bekleyip tekrar deneyin. (Hata Kodu: 429)"
            else:
                return f"Cevap Ã¼retilirken bir sunucu hatasÄ± oluÅŸtu. (Hata Kodu: {e.response.status_code})"
        except requests.exceptions.RequestException as e:
            # Genel aÄŸ/baÄŸlantÄ± hatalarÄ±nÄ± yakala
            print(f"âŒ LLM API'sine baÄŸlanÄ±rken aÄŸ hatasÄ± oluÅŸtu: {e}")
            return "Cevap Ã¼retilirken bir aÄŸ sorunu oluÅŸtu. LÃ¼tfen daha sonra tekrar deneyin."
        except json.JSONDecodeError:
            print(f"âŒ LLM API'sinden gelen yanÄ±t JSON formatÄ±nda deÄŸil. YanÄ±t: {response.text}")
            return "Cevap Ã¼retilirken modelden beklenmedik bir formatta yanÄ±t alÄ±ndÄ±."

    def process_query_with_rag(self, user_query: str):
        """
        RAG mimarisini kullanarak sorguyu iÅŸler.
        """
        print(f"ğŸ” RAG ile sorgu iÅŸleniyor: '{user_query}'")
        
        relevant_chunks = self.knowledge_proc.search(user_query, k=5)

        if not relevant_chunks or "AI hafÄ±zasÄ± henÃ¼z oluÅŸturulmadÄ±" in relevant_chunks[0]:
            return "AI hafÄ±zasÄ± boÅŸ veya bir sorun oluÅŸtu. LÃ¼tfen sunucu loglarÄ±nÄ± kontrol edin."

        final_response = self._generate_answer_with_llm(user_query, relevant_chunks)
        
        return final_response

# --- Flask UygulamasÄ± ---
app = Flask(__name__)
server = None

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/api/query', methods=['POST'])
def query():
    global server
    if server is None or server.knowledge_proc.index is None:
        return jsonify({'error': 'Sunucu veya AI hafÄ±zasÄ± hazÄ±r deÄŸil.'}), 503
    
    data = request.get_json()
    user_query = data.get('query', '')
    
    if not user_query.strip():
        return jsonify({'error': 'BoÅŸ sorgu'}), 400
    
    try:
        result = server.process_query_with_rag(user_query)
        return jsonify({'response': result})
    except Exception as e:
        print(f"HATA: {e}")
        return jsonify({'error': f'Sorgu iÅŸlenirken bir hata oluÅŸtu: {str(e)}'}), 500

if __name__ == '__main__':
    DATA_FOLDER = "company_data"
    server = AIAssistantServer(DATA_FOLDER)
    
    if server.knowledge_proc.index is not None:
        print("ğŸš€ Sunucu baÅŸarÄ±yla baÅŸlatÄ±ldÄ±!")
        print("ğŸŒ TarayÄ±cÄ±da http://localhost:5000 adresine gidin")
        app.run(debug=True, host='0.0.0.0', port=5000)
    else:
        print("âŒ Sunucu baÅŸlatÄ±lamadÄ±. 'company_data' klasÃ¶rÃ¼nÃ¼ ve AI hafÄ±za dosyalarÄ±nÄ± kontrol edin.")
