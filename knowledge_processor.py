#!/usr/bin/env python3
"""
Knowledge Processor - Metin ve tablo verilerini iÅŸler, anlamsal vektÃ¶rler oluÅŸturur
ve aranabilir bir FAISS indeksi (AI hafÄ±zasÄ±) inÅŸa eder.
"""
import os
import numpy as np
import pandas as pd
import faiss
from sentence_transformers import SentenceTransformer
import pickle

class KnowledgeProcessor:
    def __init__(self, model_name='paraphrase-multilingual-MiniLM-L12-v2'):
        """
        Bilgi iÅŸlemciyi baÅŸlatÄ±r ve embedding modelini yÃ¼kler.
        """
        print("ğŸ¤– Knowledge Processor baÅŸlatÄ±lÄ±yor...")
        # TÃ¼rkÃ§e'yi de destekleyen gÃ¼Ã§lÃ¼ bir model yÃ¼klÃ¼yoruz.
        self.model = SentenceTransformer(model_name)
        print(f"âœ… Embedding modeli '{model_name}' yÃ¼klendi.")
        self.index = None # FAISS vektÃ¶r indeksi
        self.chunks = []  # Orijinal metin parÃ§alarÄ±

    def _chunk_text(self, text: str, chunk_size=1000, overlap=150) -> list[str]:
        """
        Uzun bir metni, anlam bÃ¼tÃ¼nlÃ¼ÄŸÃ¼nÃ¼ koruyacak ÅŸekilde daha kÃ¼Ã§Ã¼k parÃ§alara ayÄ±rÄ±r.
        """
        if not isinstance(text, str):
            return []
            
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunks.append(text[start:end])
            start += chunk_size - overlap
        return chunks

    def process_knowledge_base(self, knowledge_base: dict, force_rebuild=False):
        """
        TÃ¼m bilgi tabanÄ±nÄ± (metinler, tablolar) iÅŸler, bir vektÃ¶r indeksi oluÅŸturur ve kaydeder.
        
        :param knowledge_base: Dosya adlarÄ±nÄ± ve iÃ§eriklerini (metin veya DataFrame) tutan sÃ¶zlÃ¼k.
        :param force_rebuild: KayÄ±tlÄ± bir indeks olsa bile yeniden oluÅŸturmaya zorlar.
        """
        index_path = 'faiss_index.bin'
        chunks_path = 'chunks.pkl'

        # EÄŸer yeniden oluÅŸturma zorlanmadÄ±ysa ve kayÄ±tlÄ± dosyalar varsa, onlarÄ± yÃ¼kle
        if not force_rebuild and os.path.exists(index_path) and os.path.exists(chunks_path):
            print("ğŸ’¾ KayÄ±tlÄ± AI hafÄ±zasÄ± (indeks ve metin parÃ§alarÄ±) bulunuyor, yÃ¼kleniyor...")
            self.index = faiss.read_index(index_path)
            with open(chunks_path, 'rb') as f:
                self.chunks = pickle.load(f)
            print("âœ… AI hafÄ±zasÄ± baÅŸarÄ±yla yÃ¼klendi.")
            return

        print("ğŸ§  Yeni bir AI hafÄ±zasÄ± oluÅŸturuluyor...")
        all_chunks = []
        
        for filename, content in knowledge_base.items():
            if isinstance(content, str): # PDF, DOCX metinleri
                text_chunks = self._chunk_text(content)
                # Her parÃ§aya nereden geldiÄŸini belirtmek iÃ§in kaynak bilgisi ekliyoruz.
                all_chunks.extend([f"Kaynak: {filename}\nÄ°Ã§erik: {chunk}" for chunk in text_chunks])

            elif isinstance(content, pd.DataFrame): # Excel, CSV verileri
                # DataFrame'in her satÄ±rÄ±nÄ± okunabilir bir metne dÃ¶nÃ¼ÅŸtÃ¼r
                for _, row in content.iterrows():
                    row_text = ", ".join([f"{col}: {val}" for col, val in row.items()])
                    all_chunks.append(f"Kaynak: {filename}\nÄ°Ã§erik: {row_text}")
        
        if not all_chunks:
            print("âš ï¸ Ä°ÅŸlenecek veri bulunamadÄ±. HafÄ±za oluÅŸturulamadÄ±.")
            return

        print(f"ğŸ“„ Toplam {len(all_chunks)} adet metin parÃ§asÄ± (chunk) oluÅŸturuldu.")
        print("â³ VektÃ¶rler (embeddings) oluÅŸturuluyor... Bu iÅŸlem biraz zaman alabilir.")
        
        # TÃ¼m metin parÃ§alarÄ±nÄ± vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼r
        embeddings = self.model.encode(all_chunks, show_progress_bar=True)
        
        print("âš¡ FAISS vektÃ¶r indeksi oluÅŸturuluyor...")
        d = embeddings.shape[1]  # VektÃ¶r boyutu
        self.index = faiss.IndexFlatL2(d)
        self.index.add(np.array(embeddings, dtype='float32'))
        self.chunks = all_chunks

        # Gelecekte hÄ±zlÄ± yÃ¼kleme iÃ§in indeksi ve metin parÃ§alarÄ±nÄ± kaydet
        print(f"ğŸ’¾ AI hafÄ±zasÄ± '{index_path}' ve '{chunks_path}' dosyalarÄ±na kaydediliyor.")
        faiss.write_index(self.index, index_path)
        with open(chunks_path, 'wb') as f:
            pickle.dump(self.chunks, f)
            
        print("âœ… Yeni AI hafÄ±zasÄ± baÅŸarÄ±yla oluÅŸturuldu ve kaydedildi.")

    def search(self, query: str, k=5) -> list[str]:
        """
        Verilen bir sorgu iÃ§in anlamsal olarak en alakalÄ± metin parÃ§alarÄ±nÄ± bulur.
        
        :param query: KullanÄ±cÄ±nÄ±n sorduÄŸu soru.
        :param k: DÃ¶ndÃ¼rÃ¼lecek en iyi sonuÃ§ sayÄ±sÄ±.
        :return: En alakalÄ± metin parÃ§alarÄ±nÄ±n listesi.
        """
        if self.index is None:
            return ["AI hafÄ±zasÄ± henÃ¼z oluÅŸturulmadÄ±."]
            
        query_embedding = self.model.encode([query])
        distances, indices = self.index.search(np.array(query_embedding, dtype='float32'), k)
        
        # En alakalÄ± metin parÃ§alarÄ±nÄ± dÃ¶ndÃ¼r
        results = [self.chunks[i] for i in indices[0]]
        return results